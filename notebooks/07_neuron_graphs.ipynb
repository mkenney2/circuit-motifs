{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 — Neuron-Level Attribution Graphs for Gemma-2-2B\n",
    "\n",
    "**Purpose**: Generate neuron-level attribution graphs (nodes = raw MLP neurons) for the same prompts\n",
    "already analyzed with transcoder-level graphs. If the same motif signatures (coherent FFL enrichment,\n",
    "incoherent FFL depletion) appear at the neuron level, it proves these patterns are fundamental to\n",
    "how the model routes information — not artifacts of SAE feature decomposition.\n",
    "\n",
    "**Runtime**: Google Colab T4 GPU. ~30 min per prompt, ~5 hours total for 10 prompts.\n",
    "\n",
    "**Output**: 10 JSON graphs in `neuron_graphs/{category}/` matching the circuit-tracer format\n",
    "so the existing motif pipeline works unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformer-lens torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone/mount repo for src/ imports (adjust path for your setup)\n",
    "# Option A: Clone from GitHub\n",
    "# !git clone https://github.com/YOUR_USER/network-motif-analysis.git /content/repo\n",
    "# import sys; sys.path.insert(0, '/content/repo')\n",
    "\n",
    "# Option B: Upload src/neuron_graph.py and src/graph_loader.py directly\n",
    "# Then add parent to path:\n",
    "import sys\n",
    "sys.path.insert(0, '/content')  # Adjust if needed\n",
    "\n",
    "from src.neuron_graph import (\n",
    "    NeuronGraphConfig,\n",
    "    select_top_neurons,\n",
    "    compute_all_attributions,\n",
    "    build_neuron_graph_json,\n",
    "    generate_neuron_graph,\n",
    "    characterize_graph,\n",
    ")\n",
    "from src.graph_loader import load_attribution_graph, graph_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = NeuronGraphConfig(\n",
    "    model_name=\"google/gemma-2-2b\",\n",
    "    top_k=100,           # neurons per layer\n",
    "    max_layer_gap=5,     # max layer distance for edges\n",
    "    threshold_pct=95,    # keep top 5% of attributions\n",
    "    device=\"cuda\",\n",
    ")\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gemma-2-2b\",\n",
    "    device=\"cuda\",\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(f\"Model: {model.cfg.model_name}\")\n",
    "print(f\"Layers: {model.cfg.n_layers}\")\n",
    "print(f\"d_mlp: {model.cfg.d_mlp}\")\n",
    "print(f\"d_model: {model.cfg.d_model}\")\n",
    "print(f\"Vocab size: {model.cfg.d_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Target Prompts\n",
    "\n",
    "10 prompts: 8 matched with existing transcoder graphs + 2 neuron-only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS = [\n",
    "    # Matched with transcoder graphs\n",
    "    {\"slug\": \"count-by-sevens\",     \"category\": \"arithmetic\",      \"prompt\": \"7, 14, 21, 28, 35,\"},\n",
    "    {\"slug\": \"five-plus-three\",     \"category\": \"arithmetic\",      \"prompt\": \"5 + 3 =\"},\n",
    "    {\"slug\": \"capital-france\",      \"category\": \"factual_recall\",  \"prompt\": \"The capital of France is\"},\n",
    "    {\"slug\": \"opposite-small\",      \"category\": \"factual_recall\",  \"prompt\": \"The opposite of small is\"},\n",
    "    {\"slug\": \"capital-state-dallas\",\"category\": \"multihop\",        \"prompt\": \"Dallas is a city in the state of Texas. The capital of Texas is\"},\n",
    "    {\"slug\": \"currency-france\",     \"category\": \"multihop\",        \"prompt\": \"France is a country in Europe. The currency used in France is the\"},\n",
    "    {\"slug\": \"medical-diagnosis\",   \"category\": \"reasoning\",       \"prompt\": \"A patient presents with fever, stiff neck, and headache. The most likely diagnosis is\"},\n",
    "    {\"slug\": \"sally-school\",        \"category\": \"reasoning\",       \"prompt\": \"Sally went to school. After school, Sally went to\"},\n",
    "    # Neuron-only (no transcoder match)\n",
    "    {\"slug\": \"color-sky\",           \"category\": \"factual_recall\",  \"prompt\": \"The color of the sky is\"},\n",
    "    {\"slug\": \"two-times-seven\",     \"category\": \"arithmetic\",      \"prompt\": \"2 * 7 =\"},\n",
    "]\n",
    "\n",
    "print(f\"{len(TARGETS)} target prompts:\")\n",
    "for t in TARGETS:\n",
    "    print(f\"  [{t['category']:15s}] {t['slug']:25s} → {t['prompt']!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pilot Run — Single Prompt\n",
    "\n",
    "Test on \"5 + 3 =\" to verify the pipeline works before committing to the full run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot = TARGETS[1]  # five-plus-three\n",
    "pilot_dir = Path(\"neuron_graphs\") / \"pilot\"\n",
    "pilot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Pilot: {pilot['slug']} — {pilot['prompt']!r}\")\n",
    "t0 = time.time()\n",
    "\n",
    "pilot_path = generate_neuron_graph(\n",
    "    model=model,\n",
    "    prompt=pilot[\"prompt\"],\n",
    "    slug=pilot[\"slug\"],\n",
    "    category=pilot[\"category\"],\n",
    "    output_dir=pilot_dir,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"Generated in {elapsed:.0f}s ({elapsed/60:.1f} min)\")\n",
    "print(f\"Saved to: {pilot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify JSON loads through existing pipeline\n",
    "with open(pilot_path) as f:\n",
    "    pilot_json = json.load(f)\n",
    "\n",
    "g = load_attribution_graph(pilot_path)\n",
    "print(f\"\\nLoaded graph: {g.vcount()} nodes, {g.ecount()} edges\")\n",
    "print(f\"Directed: {g.is_directed()}\")\n",
    "print(f\"Feature types: {set(g.vs['feature_type'])}\")\n",
    "print(f\"Layers: {sorted(set(g.vs['layer']))}\")\n",
    "print(f\"\\nGraph summary:\")\n",
    "for k, v in graph_summary(g).items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Characterize Pilot Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = characterize_graph(pilot_json)\n",
    "print(\"Graph properties:\")\n",
    "for k, v in props.items():\n",
    "    if k != \"nodes_per_layer\":\n",
    "        print(f\"  {k:25s}: {v}\")\n",
    "    else:\n",
    "        print(f\"  {k:25s}: {len(v)} layers\")\n",
    "\n",
    "print(\"\\nComparison with typical transcoder graphs:\")\n",
    "print(f\"  {'Property':20s} {'Transcoder':>12s} {'Neuron':>12s}\")\n",
    "print(f\"  {'Nodes':20s} {'883-1886':>12s} {props['n_nodes']:>12d}\")\n",
    "print(f\"  {'Edges':20s} {'29K-91K':>12s} {props['n_edges']:>12d}\")\n",
    "print(f\"  {'Density':20s} {'0.02-0.03':>12s} {props['density']:>12.4f}\")\n",
    "print(f\"  {'Degree Gini':20s} {'high (hubs)':>12s} {props['degree_gini']:>12.4f}\")\n",
    "print(f\"  {'Excitatory %':20s} {'80-90%':>12s} {props['excitatory_fraction']*100:>11.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Null Model Viability Check\n",
    "\n",
    "Run 10 iterations of layer-pair config null to verify std > 0 for all motifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.unrolled_census import fast_unrolled_counts\n",
    "from src.unrolled_motifs import build_catalog, get_effective_layer\n",
    "from collections import defaultdict\n",
    "\n",
    "# Layer-pair config null model (inline for Colab)\n",
    "def _lpc_null(graph, rng):\n",
    "    \"\"\"Quick layer-pair config null for viability check.\"\"\"\n",
    "    layers = [get_effective_layer(graph, v.index) for v in graph.vs]\n",
    "    pair_edges = defaultdict(list)\n",
    "    for e in graph.es:\n",
    "        key = (layers[e.source], layers[e.target])\n",
    "        pair_edges[key].append((e.source, e.target, e[\"sign\"], e[\"weight\"], e[\"raw_weight\"]))\n",
    "\n",
    "    import igraph as ig\n",
    "    g = ig.Graph(n=graph.vcount(), directed=True)\n",
    "    for attr in graph.vs.attributes():\n",
    "        g.vs[attr] = graph.vs[attr]\n",
    "\n",
    "    all_edges, all_signs, all_weights, all_raw = [], [], [], []\n",
    "    for (sl, tl), edges in pair_edges.items():\n",
    "        cur = list(edges)\n",
    "        np_ = len(cur)\n",
    "        if np_ < 2:\n",
    "            for ed in cur:\n",
    "                all_edges.append((ed[0], ed[1]))\n",
    "                all_signs.append(ed[2]); all_weights.append(ed[3]); all_raw.append(ed[4])\n",
    "            continue\n",
    "        eset = {(e[0], e[1]) for e in cur}\n",
    "        for _ in range(np_ * 10):\n",
    "            i1, i2 = rng.choice(np_, size=2, replace=False)\n",
    "            s1, t1 = cur[i1][0], cur[i1][1]\n",
    "            s2, t2 = cur[i2][0], cur[i2][1]\n",
    "            if s1 == s2 or t1 == t2: continue\n",
    "            if (s1, t2) in eset or (s2, t1) in eset: continue\n",
    "            eset.discard((s1, t1)); eset.discard((s2, t2))\n",
    "            eset.add((s1, t2)); eset.add((s2, t1))\n",
    "            a1, a2 = cur[i1][2:], cur[i2][2:]\n",
    "            cur[i1] = (s1, t2) + a1; cur[i2] = (s2, t1) + a2\n",
    "        for ed in cur:\n",
    "            all_edges.append((ed[0], ed[1]))\n",
    "            all_signs.append(ed[2]); all_weights.append(ed[3]); all_raw.append(ed[4])\n",
    "\n",
    "    g.add_edges(all_edges)\n",
    "    g.es[\"sign\"] = all_signs\n",
    "    g.es[\"weight\"] = all_weights\n",
    "    g.es[\"raw_weight\"] = all_raw\n",
    "    return g\n",
    "\n",
    "# Run viability check\n",
    "real_counts = fast_unrolled_counts(g)\n",
    "print(\"Real counts:\")\n",
    "for name, count in real_counts.items():\n",
    "    print(f\"  {name:35s}: {count:>8d}\")\n",
    "\n",
    "N_CHECK = 10\n",
    "null_counts = defaultdict(list)\n",
    "for i in range(N_CHECK):\n",
    "    rng = np.random.default_rng(seed=i)\n",
    "    g_null = _lpc_null(g, rng)\n",
    "    nc = fast_unrolled_counts(g_null)\n",
    "    for name, count in nc.items():\n",
    "        null_counts[name].append(count)\n",
    "\n",
    "print(f\"\\nNull model viability ({N_CHECK} iterations):\")\n",
    "print(f\"  {'Motif':35s} {'real':>8s} {'mean_null':>10s} {'std_null':>10s} {'viable?':>8s}\")\n",
    "for name in real_counts:\n",
    "    arr = np.array(null_counts[name], dtype=float)\n",
    "    m, s = arr.mean(), arr.std()\n",
    "    viable = \"YES\" if s > 1e-10 else \"DEGEN\"\n",
    "    print(f\"  {name:35s} {real_counts[name]:>8d} {m:>10.1f} {s:>10.2f} {viable:>8s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Full Generation Loop\n",
    "\n",
    "Generate all 10 neuron graphs. Each takes ~30 min on T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"neuron_graphs\")\n",
    "results_log = []\n",
    "total_t0 = time.time()\n",
    "\n",
    "for i, target in enumerate(TARGETS):\n",
    "    slug = target[\"slug\"]\n",
    "    category = target[\"category\"]\n",
    "    prompt = target[\"prompt\"]\n",
    "    out_dir = base_dir / category\n",
    "\n",
    "    # Skip if already generated\n",
    "    out_path = out_dir / f\"{slug}.json\"\n",
    "    if out_path.exists():\n",
    "        print(f\"[{i+1}/{len(TARGETS)}] {slug} — already exists, skipping\")\n",
    "        results_log.append({\"slug\": slug, \"status\": \"skipped\"})\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[{i+1}/{len(TARGETS)}] {slug} ({category})\")\n",
    "    print(f\"  Prompt: {prompt!r}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        path = generate_neuron_graph(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            slug=slug,\n",
    "            category=category,\n",
    "            output_dir=out_dir,\n",
    "            config=config,\n",
    "        )\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        # Quick validation\n",
    "        with open(path) as f:\n",
    "            gj = json.load(f)\n",
    "        props = characterize_graph(gj)\n",
    "        g_check = load_attribution_graph(path)\n",
    "\n",
    "        print(f\"  Done in {elapsed:.0f}s ({elapsed/60:.1f} min)\")\n",
    "        print(f\"  Nodes: {props['n_nodes']}, Edges: {props['n_edges']}, \"\n",
    "              f\"Density: {props['density']:.4f}, Gini: {props['degree_gini']:.3f}\")\n",
    "\n",
    "        results_log.append({\n",
    "            \"slug\": slug, \"status\": \"ok\", \"time_s\": elapsed,\n",
    "            **{k: v for k, v in props.items() if k != \"nodes_per_layer\"},\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  FAILED after {elapsed:.0f}s: {e}\")\n",
    "        results_log.append({\"slug\": slug, \"status\": \"error\", \"error\": str(e)})\n",
    "\n",
    "    # Print progress\n",
    "    total_elapsed = time.time() - total_t0\n",
    "    done = i + 1\n",
    "    remaining = (len(TARGETS) - done) * (total_elapsed / done)\n",
    "    print(f\"  Progress: {done}/{len(TARGETS)}, \"\n",
    "          f\"{total_elapsed/60:.0f} min elapsed, ~{remaining/60:.0f} min remaining\")\n",
    "\n",
    "    # Clear GPU cache between prompts\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "total_elapsed = time.time() - total_t0\n",
    "print(f\"\\n\\nTotal time: {total_elapsed/60:.1f} min ({total_elapsed/3600:.1f} hours)\")\n",
    "print(f\"\\nResults summary:\")\n",
    "for r in results_log:\n",
    "    print(f\"  {r['slug']:25s} {r['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download\n",
    "\n",
    "Zip the generated graphs for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generation log\n",
    "with open(base_dir / \"generation_log.json\", \"w\") as f:\n",
    "    json.dump(results_log, f, indent=2)\n",
    "\n",
    "# Zip for download\n",
    "shutil.make_archive(\"neuron_graphs\", \"zip\", \".\", \"neuron_graphs\")\n",
    "print(f\"Created neuron_graphs.zip\")\n",
    "print(f\"Download and extract to data/neuron/gemma-2-2b/ in your local repo.\")\n",
    "\n",
    "# In Colab, this triggers a download:\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"neuron_graphs.zip\")\n",
    "except ImportError:\n",
    "    print(\"Not in Colab — download neuron_graphs.zip manually.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}